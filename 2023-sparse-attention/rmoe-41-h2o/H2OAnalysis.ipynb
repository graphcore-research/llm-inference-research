{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import copy\n",
    "from typing import Iterator\n",
    "\n",
    "import tabulate\n",
    "from torch import nn\n",
    "\n",
    "import llminference as L\n",
    "import modify_gptneox as h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/nethome/douglaso/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4056154747b3491f874725265912d4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /nethome/douglaso/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-03400b5d173ab796.arrow\n",
      "Loading cached shuffled indices for dataset at /nethome/douglaso/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-c374d8616db3e9e0.arrow\n"
     ]
    }
   ],
   "source": [
    "adapter = L.Adapter.from_pretrained(\"EleutherAI/pythia-1b\")\n",
    "original_model = adapter.model\n",
    "data = L.qa.SQuAD.data()\n",
    "examples = [L.qa.add_few_shot_prompt(data[i], k=1) for i in range(20)]\n",
    "out = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EleutherAI/pythia-1b: 100%|██████████| 2/2 [02:00<00:00, 60.08s/it]\n"
     ]
    }
   ],
   "source": [
    "adapter.model = original_model\n",
    "out[\"baseline\"] = list(L.qa.evaluate(adapter, examples, batch_size=10, use_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EleutherAI/pythia-1b: 100%|██████████| 2/2 [01:55<00:00, 57.75s/it]\n"
     ]
    }
   ],
   "source": [
    "adapter.model = L.eviction_attention.convert_gptneox(\n",
    "    original_model, L.eviction_attention.Settings(k=512, local_k=256),\n",
    ")\n",
    "out[\"ours\"] = list(L.qa.evaluate(adapter, examples, batch_size=10, use_cache=False, generation_context=L.eviction_attention.generation_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EleutherAI/pythia-1b: 100%|██████████| 20/20 [01:49<00:00,  5.46s/it]\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def h20_generation_context(model: nn.Module) -> Iterator[nn.Module]:\n",
    "    yield model\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, h2o.GPTNeoXAttention_Mask):\n",
    "            m._reset_masks()\n",
    "\n",
    "adapter.model = copy.deepcopy(original_model)\n",
    "adapter.model.config.heavy_ratio = 512/2048\n",
    "adapter.model.config.recent_ratio = 256/2048\n",
    "h2o.convert_kvcache_gpt_neox_heavy_recent(adapter.model, adapter.model.config)\n",
    "adapter.model.load_state_dict(original_model.state_dict(), strict=False)\n",
    "out[\"h2o\"] = list(L.qa.evaluate(adapter, examples, batch_size=1, use_cache=False, generation_context=h20_generation_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall exact-match results:\n",
      "   baseline: 8/20\n",
      "   ours: 3/20\n",
      "   h2o: 3/20\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall exact-match results:\")\n",
    "for name, results in out.items():\n",
    "    print(f\"   {name}: {sum(x['match'] for x in results)}/{len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    baseline                 ours                     h2o                      ours_matches_h20\n",
      "--  -----------------------  -----------------------  -----------------------  ------------------\n",
      " 0  ' steam turbines\\nQues'  ' coal, oil, natural '   ' coal, oil, natural '   True\n",
      " 1  ' 1 July 1851\\nQuestio'  ' 1851\\nQuestion: What'  ' 1851\\nQuestion: What'  True\n",
      " 2  ' English MPs can onl'   ' yes\\nQuestion: MPs r'  ' yes\\nQuestion: MPs r'  True\n",
      " 3  ' Nepalese\\nQuestion: '  ' Nepalese\\nQuestion: '  ' Nepalese\\nQuestion: '  True\n",
      " 4  ' The French killed m'   ' French scounting pa'   ' French scounting pa'   True\n",
      " 5  ' Committees\\nQuestion'  ' The City Council is'   ' The City Council is'   True\n",
      " 6  ' The Mission Council'   ' The Mission Council'   ' The Mission Council'   True\n",
      " 7  ' Chest pains\\nQuestio'  ' 2 a.m\\nQuestion: Wha'  ' 2 a.m\\nQuestion: Wha'  True\n",
      " 8  ' Newton\\nQuestion: Wh'  ' Miller\\nQuestion: Wh'  ' Newton\\nQuestion: Wh'  False\n",
      " 9  ' The United States\\nQ'  ' The United States\\nQ'  ' The United States\\nQ'  True\n",
      "10  ' it is a reactive pa'   ' Ozone is a reactive'   ' Ozone is a reactive'   True\n",
      "11  \" the Second Doctor's\"   \" the Doctor's first \"   ' the Second Doctor\\nQ'  False\n",
      "12  ' Mahmud Fami Naqrash'   ' Al-Banna\\nQuestion: '  ' Al-Banna\\nQuestion: '  True\n",
      "13  ' The methodology use'   ' The Oxfam report is'   ' The Oxfam report is'   True\n",
      "14  ' About 86%\\nQuestion:'  ' About half of it.\\nQ'  ' About half of it.\\nQ'  True\n",
      "15  ' sin\\nQuestion: What '  ' sin\\nQuestion: What '  ' sin\\nQuestion: What '  True\n",
      "16  ' RNA silencing\\nQuest'  ' Viral interference\\n'  ' Viral interference\\n'  True\n",
      "17  ' Nitrogen\\nQuestion: '  ' Nitrogen\\nQuestion: '  ' Nitrogen\\nQuestion: '  True\n",
      "18  ' The Normans institu'   ' The Normans institu'   ' The Normans institu'   True\n",
      "19  ' Arriva North East\\nQ'  ' Stagecoach North Ea'   ' Stagecoach North Ea'   True\n"
     ]
    }
   ],
   "source": [
    "chars = 20\n",
    "print(tabulate.tabulate([\n",
    "    dict(baseline=repr(b[\"output\"][:chars]),\n",
    "         ours=repr(o[\"output\"][:chars]),\n",
    "         h2o=repr(h[\"output\"][:chars]),\n",
    "         ours_matches_h20=o[\"output\"][:chars] == h[\"output\"][:chars])\n",
    "    for b, o, h in zip(out[\"baseline\"], out[\"ours\"], out[\"h2o\"])\n",
    "], headers=\"keys\", showindex=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
