{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import llminference as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KV caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook showcases how to use the HuggingFace interface to generate and load the KV cache when generating new tokens, as well as how to use the custom functions to save and load the KV cache from the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = L.Adapter.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test *just* caching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try calling the model, generating cache, then calling the model w and w/o cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "# Convert to double to get rid of numerical errors\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"He was walking down the street when he saw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_tok = tokenizer(s, return_tensors=\"pt\")\n",
    "input_ids = inp_tok[\"input_ids\"]\n",
    "attention_mask = inp_tok[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into preceding context and last token\n",
    "input_ids_pre = input_ids[:, :-1]\n",
    "input_ids_end = input_ids[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the prefix to the model to generate KV cache\n",
    "past_key_values = model(input_ids_pre).past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output using the full input (no caching)\n",
    "out_no_cache = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output using the last token and KV cache\n",
    "out_cache = model(input_ids_end, past_key_values=past_key_values, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50304])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_cache.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 50304])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_no_cache.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(out_cache.logits[:, -1:, :], out_no_cache.logits[:, -1:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the KV caches\n",
    "for i, (layer1, layer2) in enumerate(zip(out_no_cache.past_key_values, out_cache.past_key_values)):\n",
    "    for kv1, kv2 in zip(layer1, layer2):\n",
    "        torch.testing.assert_close(kv1, kv2, atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The shapes of logits is different in the two situations while the past key values will be the same!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left padding with *position_ids*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass explicit *position_ids* to the modell call when using left padding to keep track of the correct position in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter = L.Adapter.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "adapter.model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"He was walking down the street\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "inp = adapter.tokenizer(s, return_tensors=\"pt\")\n",
    "seq_len = inp[\"input_ids\"].shape[-1]\n",
    "print(seq_len)\n",
    "inp_pre = {k: v[:, :-1] for k, v in inp.items()}\n",
    "inp_end = {k: v[:, -1:] for k, v in inp.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the first part of the input to the model, generate KV cache\n",
    "pkv = adapter.model(**inp_pre).past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the expected output using second part of the input + KV cache\n",
    "out1 = adapter.model(**inp_end, past_key_values=pkv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left pad the KV cache\n",
    "pad_size = 5\n",
    "pkv_padded = [[F.pad(kv, (0, 0, pad_size, 0)) for kv in layer] for layer in pkv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_len = seq_len + pad_size\n",
    "attention_mask = (torch.arange(padded_len) >= (pad_size)).long()[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output using the left-padded KV cache\n",
    "out2 = adapter.model(input_ids=inp_end[\"input_ids\"], attention_mask=attention_mask, past_key_values=pkv_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 50303 / 50304 (100.0%)\nGreatest absolute difference: 3.7517726443015817 at index (0, 0, 22995) (up to 1e-07 allowed)\nGreatest relative difference: 0.005994990867981176 at index (0, 0, 35973) (up to 1e-07 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Breaks because of wrong position ids\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_close(out1\u001b[39m.\u001b[39;49mlogits, out2\u001b[39m.\u001b[39;49mlogits)\n",
      "File \u001b[0;32m~/code/research-llm-inference/.venv/lib/python3.8/site-packages/torch/testing/_comparison.py:1511\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1489\u001b[0m error_metas \u001b[39m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1490\u001b[0m     actual,\n\u001b[1;32m   1491\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m     msg\u001b[39m=\u001b[39mmsg,\n\u001b[1;32m   1507\u001b[0m )\n\u001b[1;32m   1509\u001b[0m \u001b[39mif\u001b[39;00m error_metas:\n\u001b[1;32m   1510\u001b[0m     \u001b[39m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mraise\u001b[39;00m error_metas[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 50303 / 50304 (100.0%)\nGreatest absolute difference: 3.7517726443015817 at index (0, 0, 22995) (up to 1e-07 allowed)\nGreatest relative difference: 0.005994990867981176 at index (0, 0, 35973) (up to 1e-07 allowed)"
     ]
    }
   ],
   "source": [
    "# Breaks because of wrong position ids\n",
    "torch.testing.assert_close(out1.logits, out2.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix by passing the right position id for the last input\n",
    "out_fix = adapter.model(input_ids=inp_end[\"input_ids\"], attention_mask=attention_mask, past_key_values=pkv_padded, position_ids=torch.tensor([[seq_len-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(out1.logits, out_fix.logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading KV cache from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctxs = [\"How are you\", \"She was walking down the street\"]\n",
    "questions = [\" doing\", \" and\"]\n",
    "\n",
    "\n",
    "inps = [\n",
    "    adapter.tokenizer(ctx + question, return_tensors=\"pt\")\n",
    "    for ctx, question in zip(ctxs, questions)\n",
    "]\n",
    "\n",
    "num_new_tokens = 3\n",
    "outs = [\n",
    "    adapter.model.generate(\n",
    "        **inp, max_length=inp[\"input_ids\"].shape[1] + num_new_tokens, pad_token_id=0\n",
    "    )[:, -num_new_tokens:]\n",
    "    for inp in inps\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  32,  187,  187],\n",
       "        [ 703,  369, 9398]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(outs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'?\\n\\n'\n",
      "' she was wearing'\n"
     ]
    }
   ],
   "source": [
    "for out in outs:\n",
    "    print(repr(adapter.tok_decode(out.squeeze())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter.generate_kv_cache(ctxs, \"../../cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  32,  187,  187],\n",
       "        [ 703,  369, 9398]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter.greedy_sample(ctxs, questions, num_new_tokens, use_cache=True, cache_dir=\"../../cache\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV cache size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the expected size of the KV cache for different models and context lengths. Note, although the model's config specifies `float16`, this is the `dtype` that was used during training. When retrieving the model, the model's `dtype` is actually set to `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"pythia-70m\",\n",
    "    \"pythia-160m\",\n",
    "    \"pythia-410m\",\n",
    "    \"pythia-1b\",\n",
    "    \"pythia-1.4b\",\n",
    "    \"pythia-2.8b\",\n",
    "    \"pythia-6.9b\",\n",
    "    \"pythia-12b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_per_dtype = {torch.float16: 2, torch.float32: 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"EleutherAI/\" + \"pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXConfig {\n",
       "  \"_name_or_path\": \"EleutherAI/pythia-70m\",\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoXForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 512,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 2048,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neox\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"rotary_emb_base\": 10000,\n",
       "  \"rotary_pct\": 0.25,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.30.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_parallel_residual\": true,\n",
       "  \"vocab_size\": 50304\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 2048\n",
    "kv_sizes = {}\n",
    "for model in models:\n",
    "    config = AutoConfig.from_pretrained(\"EleutherAI/\" + model)\n",
    "    num_layers = config.num_hidden_layers\n",
    "    sequence_len = SEQUENCE_LENGTH\n",
    "    hidden_size = config.hidden_size\n",
    "    num_bytes = bytes_per_dtype[torch.float32] # since the model is returned in float32!\n",
    "    kv_size = 2 * num_layers * sequence_len * hidden_size * num_bytes\n",
    "    kv_sizes[model] = kv_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assuming sequence length 2048, each example is expected to be of size:\n",
      "\n",
      "pythia-70m: 50,331,648\n",
      "pythia-160m: 150,994,944\n",
      "pythia-410m: 402,653,184\n",
      "pythia-1b: 536,870,912\n",
      "pythia-1.4b: 805,306,368\n",
      "pythia-2.8b: 1,342,177,280\n",
      "pythia-6.9b: 2,147,483,648\n",
      "pythia-12b: 3,019,898,880\n"
     ]
    }
   ],
   "source": [
    "print(f\"Assuming sequence length {SEQUENCE_LENGTH}, each example is expected to be of size:\\n\")\n",
    "for k, v in kv_sizes.items():\n",
    "    print(f\"{k}: {v:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assuming sequence length 2048, expected KV cache for dataset with 225 examples:\n",
      "\n",
      "pythia-70m: 11,324,620,800\n",
      "pythia-160m: 33,973,862,400\n",
      "pythia-410m: 90,596,966,400\n",
      "pythia-1b: 120,795,955,200\n",
      "pythia-1.4b: 181,193,932,800\n",
      "pythia-2.8b: 301,989,888,000\n",
      "pythia-6.9b: 483,183,820,800\n",
      "pythia-12b: 679,477,248,000\n"
     ]
    }
   ],
   "source": [
    "num_examples = 225\n",
    "print(f\"Assuming sequence length {SEQUENCE_LENGTH}, expected KV cache for dataset with {num_examples} examples:\\n\")\n",
    "for k, v in kv_sizes.items():\n",
    "    print(f\"{k}: {v*225:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only considering models up to the specified size, total KV cache is:\n",
      "\n",
      "pythia-70m: 11,324,620,800\n",
      "pythia-160m: 45,298,483,200\n",
      "pythia-410m: 135,895,449,600\n",
      "pythia-1b: 256,691,404,800\n",
      "pythia-1.4b: 437,885,337,600\n",
      "pythia-2.8b: 739,875,225,600\n",
      "pythia-6.9b: 1,223,059,046,400\n",
      "pythia-12b: 1,902,536,294,400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Only considering models up to the specified size, total KV cache is:\\n\")\n",
    "size = 0\n",
    "for model in models:\n",
    "    size += kv_sizes[model] * num_examples\n",
    "    print(f\"{model}: {size:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: All tensors are returned in `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the config specifies `float16` as the `dtype`, the model is returned in `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config dtype: torch.float16\n",
      "Actual model dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Config dtype: {model.config.torch_dtype}\")\n",
    "print(f\"Actual model dtype: {model.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
